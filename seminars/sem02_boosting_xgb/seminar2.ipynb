{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "МФТИ ФИВТ: Курс Машинное Обучение (осень, 2016), Арсений Ашуха, ars.ashuha@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sudo pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import cm\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import axes3d\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "from sklearn.ensemble import GradientBoostingClassifier, AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Outline</h1> \n",
    "\n",
    "- Gardient Boosting\n",
    "- Overfitting Gardient Boosting\n",
    "    - Trees/Linear as base estimator\n",
    "    - Regularization\n",
    "    - Stohastic Gardient Boosting (Out of bag, step length)\n",
    "- Compare with bagging\n",
    "- Bossting and big number of features \n",
    "- eXtreme Gradient Boosting vs sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Gradient Boosting</h1> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Градиентный спуск\n",
    "\n",
    "Самый простой метод минимизации функции, для оптимизации в каждый момент времени двигаемся по антиградиенту функции с каким-то шагом. \n",
    "\n",
    "\n",
    "$$w_{n+1} = w_n - step \\cdot \\frac{\\partial f}{\\partial w}$$\n",
    "\n",
    "### Градиентный бустинг\n",
    "\n",
    "Теперь давайте представим, что на каждом шаге мы оптимизируем не параметры алгоритма $w$, а ответы нашего алгоритма $\\hat{y}$.\n",
    "\n",
    "**Обучение**: На каждом шаге, давайте предсказывать градиент на каждом объекте и \"двигать\" ответ в сторону улучшения (антиградиента).\n",
    "\n",
    "**Как в итоге обучать**:\n",
    "- Первый алгоритм отвечает константу \n",
    "- Добавляем базовые алгоритмы $b_i$, $i = 1, .., N$:\n",
    "    - Вычисляем градиент функции потерь ПО ОТВЕТАМ $g_{i-1} = \\frac{\\partial \\sum_{j=0}^{i-1} a_j b_j(x)}{\\partial y}$ на каждом объекте  \n",
    "    - Обучаем $b_i$ предсказывать текущий $g_{i-1}$\n",
    "    - Подбираем $a_i$ -- одномерной минимизацией \n",
    "    - Дополняем композицию $\\sum_{j=0}^{i-1} a_j b_j (x) + a_i b_i(x)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h1 align=\"center\">Задачки</h1> \n",
    "\n",
    "1) В задаче классификации с двумя классами $Y = {+1, −1}$ разумным выбором является настройка функции $p_+(x) ∈ [0, 1]$, возвращающей вероятность класса +1. В этом случае мы можем измерить правдоподобие обучающей выборк при условии модели $p_{+}(x)$:\n",
    "\n",
    "![](./img/logist1.png)\n",
    "![](./img/logist2.png)\n",
    "\n",
    "----\n",
    "\n",
    "2) Как будет выглядеть задача поиска базового алгоритма $b_N(x)$ в случае с логистической функцией потерь?\n",
    "\n",
    "![](./img/logist3.png)\n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Overfiеting and Regularization</h1> \n",
    "\n",
    "\n",
    "- Что будет, если первый классификатор хорошо предскажет все ответы?\n",
    "- Переобучается ли бустинг в случае переобучения одного базового алгоритма?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Сгенерируем данные "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = np.arange(-5, 5, 0.25)\n",
    "Y = np.arange(-5, 5, 0.25)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "Zt = np.sin(np.sqrt(X**2 + Y**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(7, 5))\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.plot_surface(X, Y, Zt, rstride=1, cstride=1, cmap=cm.hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наложим шум"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "Z = Zt\n",
    "Z += np.random.normal(size=X.shape)*0.2\n",
    "Z += np.random.normal(size=X.shape)*0.02 \n",
    "\n",
    "fig = plt.figure(figsize=(7, 5))\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap=cm.hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тренируем AdaBoostRegressor\n",
    "\n",
    "Каков будет результат?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1 = AdaBoostRegressor(\n",
    "    base_estimator=DecisionTreeRegressor(), \n",
    "    loss='exponential').fit(np.vstack(np.array([X, Y]).T), np.hstack(Z))\n",
    "\n",
    "X = np.arange(-5, 5, 0.25)\n",
    "Y = np.arange(-5, 5, 0.25)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "Zf1 = f1.predict(np.vstack(np.array([X, Y]).T))\n",
    "\n",
    "fig = plt.figure(figsize=(7, 5))\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.plot_surface(X, Y, Zf1.reshape(40, 40), rstride=1, cstride=1, cmap=cm.hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Тренируем SVM Regressor\n",
    "\n",
    "Какой результат будет? Какое ядро стоит взять?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(url='http://scikit-learn.org/stable/_images/plot_iris_0012.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "?SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f1 = SVR(kernel='rbf').fit(np.vstack(np.array([X, Y]).T), np.hstack(Z))\n",
    "\n",
    "X = np.arange(-5, 5, 0.25)\n",
    "Y = np.arange(-5, 5, 0.25)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "Zf1 = f1.predict(np.vstack(np.array([X, Y]).T))\n",
    "\n",
    "fig = plt.figure(figsize=(7, 5))\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.plot_surface(X, Y, Zf1.reshape(40, 40), rstride=1, cstride=1, cmap=cm.hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Делаем из бустинка конфетку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "?AdaBoostRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Переобучается ли бустинг в случае переобучения одного базового алгоритма?\n",
    "- Как регуляризовывать бустинг?\n",
    "    - Регуляризуем базовые алгоритмы\n",
    "    - $$\\sum_{j=0}^{i-1} a_j b_j (x) + v \\cdot a_i b_i(x), 0 < v < 1$$\n",
    "- Какие функции потерь использовать?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ====================================\n",
    "#       Ячейка интерактива\n",
    "# ====================================\n",
    "\n",
    "f1 = AdaBoostRegressor(\n",
    "    base_estimator=???, \n",
    "    loss=???, \n",
    "    learning_rate=???, \n",
    "    n_estimators=????\n",
    ").fit(np.vstack(np.array([X, Y]).T), np.hstack(Z))\n",
    "\n",
    "X = np.arange(-5, 5, 0.25)\n",
    "Y = np.arange(-5, 5, 0.25)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "Zf1 = f1.predict(np.vstack(np.array([X, Y]).T))\n",
    "\n",
    "fig = plt.figure(figsize=(7, 5))\n",
    "ax = fig.gca(projection='3d')\n",
    "ax.plot_surface(X, Y, Zf1.reshape(40, 40), rstride=1, cstride=1, cmap=cm.hot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Real Data</h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "if not (os.path.exists('./data/cifar10') and os.path.exists('./data/adult.data')):\n",
    "    !sh ./get_data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "adult = pd.read_csv(\n",
    "    './data/adult.data', \n",
    "    names=[\n",
    "        \"Age\", \"Workclass\", \"fnlwgt\", \"Education\", \"Education-Num\", \"Martial Status\",\n",
    "        \"Occupation\", \"Relationship\", \"Race\", \"Sex\", \"Capital Gain\", \"Capital Loss\",\n",
    "        \"Hours per week\", \"Country\", \"Target\"], \n",
    "    header=None, na_values=\"?\")\n",
    "adult = pd.get_dummies(adult)\n",
    "adult[\"Target\"] = adult[\"Target_ >50K\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X, y = adult[adult.columns[:-3]].values, adult[adult.columns[-1]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting vs Stochastic Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf1 = LogisticRegression()\n",
    "print cross_val_score(clf1, X, y, cv=4, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf1 = RandomForestClassifier()\n",
    "print cross_val_score(clf1, X, y, cv=4, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "clf1 = GradientBoostingClassifier()\n",
    "print cross_val_score(clf1, X, y, cv=4, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "?GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf1 = GradientBoostingClassifier(subsample=0.8)\n",
    "print cross_val_score(clf1, X, y, cv=4, n_jobs=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting and a lot of features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from utils import load_CIFAR10\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, y_train, X_test, y_test = load_CIFAR10('./data/cifar10/cifar-10-batches-py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 7))\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 7\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(y_train == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(X_train[idx].astype('uint8'))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test = X_train.reshape(X_train.shape[0], -1), X_test.reshape(X_test.shape[0], -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ===========================================\n",
    "#  Даже не пытйтесь это учить. Почему??!!\n",
    "# ===========================================\n",
    "\n",
    "#clf1 = GradientBoostingClassifier(subsample=0.3, learning_rate=0.05, n_estimators=10).fit(X_train, y_train)\n",
    "#print accuracy_score(clf1.predict(X_train), y_train), accuracy_score(clf1.predict(X_test), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform = PCA(n_components=100, copy=False)\n",
    "X_ = transform.fit(X_train[:2000, :])\n",
    "X_tr, X_te = transform.transform(X_train), transform.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf1 = GradientBoostingClassifier(subsample=0.8, learning_rate=0.05, n_estimators=100).fit(X_tr[:3000], y_train[:3000])\n",
    "print accuracy_score(clf1.predict(X_tr), y_train), accuracy_score(clf1.predict(X_te), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Будет ли лучше если вычесть среднюю картинку?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean = X_train.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform = PCA(n_components=100, copy=False)\n",
    "X_ = transform.fit(X_train[:2000, :]-mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_tr, X_te = transform.transform(X_train-mean), transform.transform(X_test-mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clf1 = GradientBoostingClassifier(subsample=0.8, learning_rate=0.05, n_estimators=100).fit(X_tr[:3000], y_train[:3000])\n",
    "print accuracy_score(clf1.predict(X_tr), y_train), accuracy_score(clf1.predict(X_te), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Length of dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "transform = PCA(n_components=100, copy=False)\n",
    "X_ = transform.fit(X_train[:2000, :])\n",
    "X_tr, X_te = transform.transform(X_train), transform.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in [300, 1000]:#, 3000, 5000, 10000, 20000, 40000]:\n",
    "    %time clf1 = GradientBoostingClassifier(subsample=0.8, learning_rate=0.1,n_estimators=200).fit(X_tr[:i], y_train[:i])\n",
    "    print '#num_items', i, 'test', accuracy_score(clf1.predict(X_te), y_test), '\\n'"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "CPU times: user 5.26 s, sys: 47.4 ms, total: 5.31 s\n",
    "Wall time: 5.35 s\n",
    "#num_items 300 train 0.23562 test 0.2255 \n",
    "\n",
    "CPU times: user 18.2 s, sys: 29.8 ms, total: 18.2 s\n",
    "Wall time: 18.2 s\n",
    "#num_items 1000 train 0.32046 test 0.3065 \n",
    "\n",
    "CPU times: user 1min 2s, sys: 599 ms, total: 1min 3s\n",
    "Wall time: 1min 6s\n",
    "#num_items 3000 train 0.40506 test 0.3639 \n",
    "\n",
    "CPU times: user 1min 35s, sys: 382 ms, total: 1min 36s\n",
    "Wall time: 1min 36s\n",
    "#num_items 5000 train 0.44878 test 0.3907 \n",
    "\n",
    "CPU times: user 3min 32s, sys: 1.21 s, total: 3min 33s\n",
    "Wall time: 3min 34s\n",
    "#num_items 10000 train 0.50526 test 0.4232 \n",
    "\n",
    "CPU times: user 8min 42s, sys: 5.69 s, total: 8min 48s\n",
    "Wall time: 9min 4s\n",
    "#num_items 20000 train 0.54846 test 0.4476 \n",
    "\n",
    "CPU times: user 21min 4s, sys: 16.1 s, total: 21min 20s\n",
    "Wall time: 7h 25min 30s\n",
    "#num_items 40000 train 0.57684 test 0.4601 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">eXtreme Gradient Boosting vs sklearn</h1> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "i = 5000\n",
    "clf1 = GradientBoostingClassifier(subsample=0.8, learning_rate=0.1,n_estimators=200).fit(X_tr[:i], y_train[:i])\n",
    "print 'test', accuracy_score(clf1.predict(X_te), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "?xgb.XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "i = 5000\n",
    "clf1 = xgb.XGBClassifier(subsample=0.8, learning_rate=0.1,n_estimators=200).fit(X_tr[:i], y_train[:i])\n",
    "print 'test', accuracy_score(clf1.predict(X_te), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Качество лучше, работает быстрее. Иногда проигрывает RGBM, надо пробовать. \n",
    "\n",
    "Итак, градиентный бустинг в XGBoost имеет ряд важных особенностей.\n",
    "1. Базовый алгоритм приближает направление, посчитанное с учетом вторых производных функции потерь.\n",
    "2. Отклонение направления, построенного базовым алгоритмом, измеряется с помощью модифицированного функционала — из него удалено деление на вторую производную, за счет чего избегаются численные проблемы.\n",
    "3. Функционал регуляризуется -- добавляются штрафы за количество листьев и за норму коэффициентов.\n",
    "4. При построении дерева используется критерий информативности, зависящий от оптимального вектора сдвига.\n",
    "5. Критерий останова при обучении дерева также зависит от оптимального сдвига."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "\n",
    "grid = {'n_estimators': [50, 100], 'max_depth': [4, 6]}\n",
    "gs = GridSearchCV(xgb.XGBClassifier(), grid, scoring='accuracy', cv=2, n_jobs=4)\n",
    "gs.fit(X_tr, y_train)\n",
    "\n",
    "for a in gs.grid_scores_:\n",
    "    print a.parameters, a.mean_validation_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Осталось время: контест на паре, лучшее качество бустингом на сифар"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 align=\"center\">Recap</h1> \n",
    "\n",
    "- Один из лучших методов построения композиций\n",
    "- Нужно много деревьев и желательно не тысячи признаков\n",
    "- Много хороших реализаций\n",
    "- Плохо подходит для картинок и прочих очень понятных данных\n",
    "\n",
    "**Что почитать**:\n",
    "- Hastie, The Elements of Statistical Learning, https://goo.gl/k3wfEU, 10 Boosting and Additive Trees 337\n",
    "- Соколов, Семинары по композиционным методам, https://goo.gl/sn8RyJ, http://goo.gl/ajNTQy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
